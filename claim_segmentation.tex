\chapter{Claim segmentation}
\label{chap:claim_segmentation}

Instead of dealing with entire posts as done in chapters~\ref{chap:argclu} and
~\ref{chap:argrec}, here we wish to first segment posts into \emph{atomic units
that convey a single thought} -- \textbf{claims}.
The term claim is used to refer to an \emph{elementary argumentative component}, 
which are a generalization of elementary discourse units
\citep{winter1982towards, givon1983topic, polanyi1996linguistic}
(argumentative units are 
explained in subsection~\ref{subsec:arg_comp_ext}).  Claims as
defined here are not to be confused with claims from Toulmin or Freeman's
argumentation model. Splitting text into argumentative claims is important
because of two main reasons. 
First, non-argumentative content is discarded as it does not 
make statements relevant to the topic
only (even though sometimes that can prove useful as shown by 
\citep{madnani2012identifying}).
Second, the process of claim segmentation results in claims, which are the
basis of subsequent analysis. Extracted claims can then be related to each
other (i.e. \emph{support} or \emph{attack} relations), further specialized to
fit an argument model (i.e. claims are classified into warrants, rebuttals,
etc. stemming from the Toulmin model), directly analyzed (i.e. clustered as
done in chapter~\ref{chap:argclu}), or structured (i.e. to microstructures or
ontological formalizations as explained in chapter~\ref{chap:formalization}).

%The \textbf{claim segmentation task} corresponds to the task of argumentative component
%extraction, more specifically argumentative boundary detection, which is
%explained in subsection~\ref{subsec:arg_comp_ext}. 

% TODO give intro to motivate structure approach
% outline steps of structured approach


\section{Data}

\setlength{\tabcolsep}{4pt}

\begin{table*}[t]
\begin{center}
{\footnotesize
	\begin{tabular}{@{}p{0.2\linewidth} p{0.30\linewidth} p{0.30\linewidth} }
\toprule
\textbf{User post} & \textbf{Claim segment} & \textbf{Claim paraphrase}   \\
\midrule
\multirow{3}{*}{\parbox{3cm}{
		\emph{Men should fall in love with women that's why they where
		created and women should get married to men because it makes
		everything easier. }
}}
&  
\emph{Men should fall in love with women.}
& \emph{People of opposite sex should fall in love.}
\\
\cmidrule{2-3}
& \emph{that's why they where created} & \emph{Men and women are created to pair.}
 \\
\cmidrule{2-3}
& \emph{women should get married to men because it makes everything easier.} & 
 \emph{Heterosexual marriages make everything easier.}
 \\
 \bottomrule
\end{tabular}}
\end{center}
\caption{An example of a user post segmented into three claim segments with
	their correspoding paraphrase.}
\label{tab:claim_seg_post_segments}
\end{table*}

We adopt the dataset of \citet{hasan2014you} which contains 
user posts from online two-sided discussions on a number of issues. 
We consider two topics: ``Gay Rights'' and 
``Marijuana'' and 
sample 100 posts (50 \textbf{pro} and 50 \textbf{con}) from each topic. 
For both topics, we used trained annotators.
First, annotators segment out claims from user posts 
Second, after segmenting out a claim, the annotators provide a paraphrased
version of the claim.
We assume that paraphrasing helps understanding of claims.
% TODO think how to incorpore wyner 
% Our work is similar to \citep{wyner2016working} who use a controlled language 
% for paraphrasing claims. 

Claim segmenting separates argumenative from non-argumentative content. 
There are many ways a post can be segmented into claims in addition to being
a number of ways to paraphrase a claim. 
The ambiguity can be reduced by doing these two tasks jointly. 
The end result paraphrased should be \emph{simplyfing claim paraphrases}: 
paraphrases that provide the essence of claims devoid of 
superflous words and phrases. 
To that end, we adopt nine paraphrasing principles: 
\begin{enumdescript}
\item[Argumentativeness] --- Only argumentative text should paraphrased;
\item[Atomicity] --- A claim should convey a single thought; 
\item[Authority] --- Experts in claims from expert opinion should be made
	explicit in the paraphrase; 
\item[Brevity] --- Paraphrases should keep only the relevant argumentative
	content; 
\item[Canonicity] --- Canonical terms and phrases are preffered over idiomatic
	language; 
\item[Contextuality] --- Claims should be paraphrased by considering their
	local and topical context as well as their context; 
\item[Declarativity] --- paraphrases should be in declarative form; 
\item[Dereferencing] --- Pronouns and nominal references should be resolved; and
\item[Explicitness] --- Only explicitly stated information should be
	paraphrased, and not whatever might be implied by the claim 
\end{enumdescript}
Unlike most previous approaches to argumentative component boundaries, we allow
for both overlapping and discontiguous segments.  Full annotation guidelines
are in appendix~\ref{sec:argseg_annotation}. The annotation for ``Gay Rights''
was carried out by one trained annotator and took 25 hours.
The annotation for ``Marijuana'' was carried out by three annotators. 
The 100 user posts yield 920 claim segments for ``Gay rights'', as did the 100 
user posts in ``Marijuana''. 
Table~\ref{tab:claim_seg_post_segments} gives an example of a post from
an online discussion split into segmented and paraphrased claims. 
For the ``Gay Rights'' topic, the segments covered 79.6\% of the text, while the remaining
20.4\% may be considered non-argumentative. The ``Marijuana'' topic contained 
more argumentative text, as 88.98\% of text is covered by argumentative 
segments, while 11.02\% was annotated as non-argumentative. 

% \noindent - dataset statistics \\
% - how many examples are overlapping \\

\section{Problem Formulation}
\begin{table}
\centering
\begin{tabular}{c| c c}
 i & $x_i$ & $Y_i$ \\
\midrule
 1& marijuana & $\{1 \}$ \\
 2& is & $\{ 1 \}$ \\
 3& believed &  $\{ 1 \}$ \\
 4& to& $\{ 1 \}$ \\
 5& be& $\{ 1 \}$ \\
 6& a& $\{ 1 \}$ \\
7 & stepping& $\{ 1 \}$ \\
8 & -& $\{ 1 \}$
\end{tabular}
\hfill
\begin{tabular}{c | c c}
 i & $x_i$ & $Y_i$ \\
\midrule
9 & stone& $\{ 1 \}$ \\
10& drug& $\{ 1 \} $ \\
11& that& $\{2, 3, 4\}$ \\
12& can& $\{2, 3, 4\}$ \\
13& eventually& $\{2, 3, 4\}$ \\
14& lead& $\{2, 3, 4\}$ \\
15& to& $\{2, 3, 4\}$ \\
16& addiction& $\{2, 3, 4\}$ \\
\end{tabular}
\hfill
\begin{tabular}{c| c c}
 i & $x_i$ & $Y_i$ \\
\midrule
17 & to& $\{2, 3, 4\}$ \\
18 & heroin & $\{ 2 \}$ \\
19 & , & $\{2\}$ \\
20 & cocaine& $\{3 \}$ \\
21 & and& $\{ \}$ \\
22 & other& $\{ 4 \} $\\
23 & harder& $\{ 4 \} $\\
24 & drugs& $\{ 4 \} $\\
\end{tabular}
\caption{Marijuana example}
\label{tab:multilabel_segment_example}
\end{table}


\begin{figure}
\scriptsize
\begin{tabular}{l | ccccccc cccccccc ccc ccccc}
	& Nothing& can& bring& peace& to& this& world 
&Its& a& great& idea& to& try& and& push& 
for& world& peace 
	& but & it & will & never & happen \\
	\midrule
	\texttt{BIO} & \texttt{B} & \texttt{I} & \texttt{I} &
	\texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} & \texttt{B}&
	\texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} &
	\texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} &
	\texttt{B} & \texttt{I} & \texttt{I} & \texttt{I} & \texttt{I} \\
	\midrule
	\multirow{3}{*}{ML} & 1 & 1 & 1 & 1 & 1 & 1 & 1 
	& 0& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 
	& 0 & 0 & 0 & 0 & 0 \\

	& 0 & 0 & 0 & 0 & 0 & 0 & 0 
	& 1& 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 
	& 0 & 0 & 0 & 0 & 0 \\

	& 0 & 0 & 0 & 0 & 0 & 0 & 0 
	& 0& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 
	& 1 & 1 & 1 & 1 & 1 \\

\end{tabular}
	\caption{Multilabel (ML) and \texttt{BIO} labelling of post with three segments: 
	``\textit{Nothing can bring peace to this world}'', 
	``\textit{Its a great idea to try and push for world peace}'', and
	``\textit{but it will never happen}''. 
}
\label{fig:segment_example_bio_multilabel}
\end{figure}

Now, since the dataset is defined, we will mathematically formalize this
problem.  Let $\mathbf{x} = (x_1, \dots, x_N)$ represent a sentence of size $N$
as a vector of tokens where the index represents the position of the word in
the sentence.  Then $\mathbf{Y} = (Y_1, \dots, Y_N)$ is a vector where each
element represents a set of labels.  Each $Y_i, i \in \{1, \dots, N\}$ is a set
of labels denoting which segment the the token belongs to.  As an example, the
post(comment) ``\textit{ Marijuana is believed to be a stepping-stone drug that
can eventually lead to addiction to heroin, cocaine and other harder
% TODO actually is 25 tokens
drugs.}'' is tokenized into 24 tokens with respective $X$ and $Y$ shown in
table ~\ref{tab:multilabel_segment_example}. 
Mapping $x$ to $Y$ can be framed as multilabel classification since
each $y \in Y$ can contain up to $N$ labels, yielding a total of $2^{N}$ 
possible combinations per element. 
This problem can now be framed as multilabel classification 
(multilabel classification described in section \ref{sec:chain_classification}). 
If it is framed using the binary relevance setup there is
an exponential number of possible solutions, which makes efficient search 
of possible solutions expensive. 
The third row of figure~\ref{fig:segment_example_bio_multilabel} shows how a
multilabel labeling of segments would be on a post from the ``Marijuana''
topic. 

Alternatively to multilabel classification, a number of approaches have been
explored. Looking at sequence extraction in other areas of information
extraction, problems of shallow parsing \citep{sha2003shallow} and named entity
recognition \citep{nadeau2007survey} stand out.  \citet{sha2003shallow}
employed \texttt{BIO} encoding for shallow parsing and used conditional random
fields as one of the first structured prediction approaches to information
extraction.  \texttt{BIO} labels indicate whether the word is outside a segment
(\texttt{O}), starts a segment (\texttt{B}) or continues a segment
(\texttt{I}).  In order to apply \texttt{BIO} labeling, the final segments
\emph{must} be non-overlapping and contigious. 
Row 2 of figure ~\ref{fig:segment_example_bio_multilabel} shows a
post from the ``Marijuana'' topic 
containing three segments labeled with \texttt{BIO} tags. 
Using \texttt{BIO} tags instead of applying the multilabel approach
simplifies solving the sequence extraction by 
having to predict only a single label per token, which makes the number of solutions 
for each $x$ go down from $2^{N}$ to $|\{B, I, O\}|$. 

But, when the segments are overlapping or discontiguous using \texttt{BIO} tags
can't be used to bijectively encode segments of labels. 
To tackle overlapping and/or discontiguous segments a number of approaches have been
proposed, mostly
in the area of named entity recognition on the analysis of clinical texts, 
on datasets such as BioInfer \citep{pyysalo2007bioinfer}. 
\citet{byrne2007nested} proposed a solution for 
extracting non-overlapping contigious entities which uses
tag n-grams instead of words. Working to also solve 
non-overlapping contigious entities 
\citep{alex2007recognising} propose using multiple-layer tagging. 

% TODO, tu sam stao
More recently, as part of the SemEval-2014 analysis of clinical texts competition,
extracting overlapping and contigious entities from text was explored
\citep{pradhan2014semeval}.

- \citet{finkel2009nested} treat named entity recognition as a parsing task \\
- most recently, this becase an interesting problem
as part of the SemEval-2014 analysis of clinical texts competition 
- one of the competition tasks was extraction of all medical named entities, which could have been
discontiguous and overlapping \\
- two systems in the competition produces solutions that could handle both 
discontiguous and overlapping entities \\
- first, in \citep{pathak2014ezdi} they use a standard \texttt{BIO} tagging pipelined 
with SVMs to combine resulting spans \\
- the second system, described in \citep{zhang2014uth_ccb} expands 
on the regular \texttt{BIO} tagset \\
- they propose to tag each token with one of \texttt{B}, \texttt{I},
\texttt{O}, \texttt{BD}, \texttt{ID}, \texttt{BH}, and
\texttt{IH} which denote \texttt{B}eggining of entity, \texttt{I}nside entity, \texttt{O}utside
of entity, \texttt{B}eginning of 
\texttt{D}iscontigious entity, \texttt{I}nside of \texttt{D}iscontigious entity,
\texttt{B}eginning of \texttt{H}ead, and
\texttt{I}nside of \texttt{H}ead.  \\
- but, this encoding is lossy, which makes decoding ambigous in some cases \\
- further, this encoding may produce invalid sequences \\
- this encoding is used in \citet{muis2018learning} where they propose
a hypergraph-based model \\
- most recently, \citet{dai2018recognizing} names the problem as complex 
entity mention recognition, summarizes research so far and provides
future work guidelines \\
- they mention 
multilabel encoding as the most efficient one, as it 
doesn't lose any information \\

\noindent - the clinical named entity extraction problem and semantic parsing are
equivalent to the claim segmentation problem \\ 
- we will then try two approaches: multilabel and \texttt{BIO} approach since both
are nonambigous \\
- the multilabel approach will attempt to handle discontiguous and overlapping
entities, whereas in the \texttt{BIO} we will try to predict only segments
which aren't discontiguous or overlapping \\
- a comparison of \texttt{BIO} and multilabel encoding for a segment are shown in 
figure~\ref{fig:segment_example_bio_multilabel} \\

\begin{figure}
\scriptsize
\begin{tikzpicture}[node distance=1mm]
\node (s1) {Marijuana is believed to be a stepping-stone drug};
\node (s234) [right=of s1]{that can eventually lead to additiction to};
	\node (s2) [right=of s234]{heroin\textcolor{white}{y}};
	\node (s3) [right=of s2.east]{cocaine\textcolor{white}{y}};
	\node (o) [right=of s3] {and\textcolor{white}{y}};
	\node (s4) [right=of o] {other\textcolor{white}{y}harder drugs.};

\node (s1lab) [below=of s1] {\textcolor{red}{Segment 1}};
\node (s2lab) [below=of s2, xshift=-1.7cm, yshift=-2.5mm] {\textcolor{blue}{Segment 2}};
\node (s3lab) [below left=1cm of s3, xshift=1mm, yshift=-1mm] {\textcolor{green}{Segment 3}};
\node (s4lab1) [below=of s234, xshift=-0.6cm, yshift=1mm] {\textcolor{orangegreen}{Segment 4}};
\node (s4lab2) [below=of s4] {\textcolor{orangegreen}{Segment 4}};

\draw [-]
($(s1.east) - (0.5mm, 0)$)
-- ++(0, -.2)
-| ($(s1.west) $);

% S4 segment
\draw [-]
($(s234.east) $)
-- ++(0, -0.2)
-| ($(s234.west) - (0, 0) $);

% S2 segment
\draw [-]
($(s234.west) - (0.5mm, 0)$)
-- ++(0, -0.6)
	-| ($(s2.east) - (0.5mm, 0) $);

% S3 segment
\draw [-]
($(s234.west) - (1mm, 0)$)
-- ++(0, -1.0)
-| ($(s3.east) $);

% other S4 segment
\draw [-]
($(s4.west) $)
-- ++(0, -.2)
-| ($(s4.east) $);
\end{tikzpicture}
	\caption{An example of post made in the ``Marijuana'' topic. 
	\textcolor{red}{Segment 1} is a regular segment. 
	\textcolor{blue}{Segment 2} and \textcolor{green}{Segment 3} are mutually overlapping.
	\textcolor{orangegreen}{Segment 4} is both overlapping and discontiguous.}
	\label{fig:segment_example_range}
\end{figure}

%'<S1>Marijuana is believed to be a stepping-stone drug
%</S1><S2><S3><S4>that can eventually lead to addiction to
%</S3></S4>heroin,</S2><S3> cocaine</S3> and<S4> other harder
%drugs.</S4>'

% <S1>Nothing can bring peace to this earth.</S1> <S2>Its a great idea to try and
% push for world peace</S2> <S3>but it will never happen...</S3>'

\subsection{Models}

\subsubsection{Na\"ive Heuristic}

\begin{figure}
\begin{algorithmic}[1]
\Function{is\_punctuation}{ch}
\If{$ch = ``." \cup ch = ``?" \cup ch = ``!"$}
\State
\Return $\mathit{True}$
\Else
\State
\Return $\mathit{False}$
\EndIf
%\Return $out$
\EndFunction

\State
\State 

\State \textbf{INPUT} $\mathit{text}$
\State $\mathit{seg\_id} \gets 1$
\State $Y[\mathit{seg\_id}] \gets \emptyset$
\ForAll{$i \in |\mathit{text}|$}
	\State $token \gets text[i]$
	\State $Y[\mathit{seg\_id}] \gets Y[\mathit{seg\_id}] \cup \mathit{token}$
	\State
	\If{$\mathit{IS\_PUNCTUATION}(\mathit{token}) \cap i \neq 0$}
	\State $\mathit{seg\_id} \gets \mathit{seg\_id} + 1$
	\EndIf
\EndFor
\State
\Return $Y$
\end{algorithmic}
\caption{Claim segmentation punctuation based heuristic. 
Input is a list of tokens, outputs a nested list of lists, where 
each list corresponds to a segment. 
None of the tokens is declared as non argumentative.
The \texttt{IS\_PUNCTUATION}
	function has been simplified for readability purposes. 
	}
\label{alg:heuristic_claimseg}
\end{figure}

- similarly to \citep{ajjour2017unit}, we follow existing approaches to tackle
claim segmentation as a token-level classification task \\
- we observe the claim segmentation problem in two lights \\ 
- first, as a multilabel classification problem, since each token can belong to zero or more
segments \\
- second, we simplify the problem by discarding overlapping and discontiguous segments \\
- we compare three basic approaches \\
- first approach is based on most argumentation mining approaches where 
- in the second approach we use an SVM classifier to use word level and context features \\
- in the third approach, we combine deep learning and 
structured prediction \\

\noindent As a strong baseline, we extract claims as most papers assume claim 
segmentation has already been performed, some assume a single sentence is a claim
\citep{rooney2012applying, stab2014identifying}. \\
- We adopt a similar approach, where we simply assume punctuation is a strong
indication a new claim starts. \\
- For each post (comment), we simply iterate through all tokens
and start a new claim when a token equals a punctuation mark \\
- this is essentially similar to sentence segmentation \citep{palmer2000tokenisation} \\
- If the current token is a punctuation symbol, the next claim starts \\
The heuristic is outlined in algorithm~\ref{alg:heuristic_claimseg} \\
- An example of how the algorithm works for a post is shown in~\ref{tab:heuristic_example} \\

\begin{table}[h]
\begin{tabular}{@{}p{0.5\linewidth} p{0.50\linewidth}}
\toprule
\multirow{7}{*}{
\parbox{7cm}{
\textit{if we legalize pot there wil be a sharp increase in 
demand and consumption over a period of time
then............it will start to decline....slowly at first until
smoking pot becomes drinking wine.......only on special
occasions........... that's why we should continue the war
it's not like weed is damn near impossible to obtain ...... in my
opinion there is no war ....... there never was ...}
}}
		& \emph{if we legalize pot there wil be a sharp increase in 
demand and consumption over a period of time
	then ............} \\ \cline{2-2}
		& \emph{it will start to decline .... } \\ \cline{2-2}
		& \emph{slowly at first until
	smoking pot becomes drinking wine .......} \\  \cline{2-2} 
	& \emph{
only on special
occasions
...........
		} \\  \cline{2-2}
	& \emph{
that's why we should continue the war
it's not like weed is damn near impossible to obtain ......
		} \\\cline{2-2}
	& \emph{
in my
opinion there is no war .......
} \\\cline{2-2}
	& \emph{
there never was ...
} \\
\bottomrule
	\caption{Example of heuristic algorithm output. }
\label{tab:heuristic_example}
\end{tabular}
\end{table}

\subsubsection{SVM}

- We employ Support Vector Machines \\
- we use a weighted svm where weights are calculated to compensate for the class
disbalance by weighing less prominent class examples with the ratio of number of 
dominant class vs. recesive class examples in the training set \\
- use $5 \times 3$ nested-cross validation \\
- in the multilabel approach, we create training examples by taking a word with a
configurable context window (of surrounding words) \\

predicting each label  \\
- we experiment with tf-idf features and 
distributed word representations (fast text and word2vec pretrained vectors) 


\subsubsection{BiLSTM + CRF Model}


\section{Experiments and Discussion}

\begin{figure}
\begin{algorithmic}[1]
\State \textbf{INPUT} $\mathit{predicted}, \mathit{true}, \mathit{offset}$
\State $\mathit{found} \gets \{\}$
\State
\ForAll{$i \in |\mathit{true}|$}
  \ForAll{$j \in |\mathit{predicted}|$}
	\State $\mathit{common} \gets $\texttt{LCS\_LENGTH}$(\mathit{true}[i], \mathit{predicted}[j])$
	\State
	\State $\mathit{is\_smaller\_than\_offset} \gets |\mathit{predicted}[j]| > \mathit{offset}$
	\State $\mathit{is\_common\_enough} \gets $ $|\mathit{predicted}[j]| -
	\mathit{common} \leq \mathit{offset}$
	\State $\mathit{has\_small\_size\_diff} \gets |\mathit{predicted}[j]| -
	|\mathit{true}[i]| \leq \mathit{offset}$
	\State
	\If{$\mathit{is\_smaller\_than\_offset} \cap 
	\mathit{is\_common\_enough} \cap \mathit{has\_small\_size\_diff}$}
	  \State $\mathit{found} \gets \mathit{found} \cup \mathit{true}[i]$
	  \State \textbf{break}
	\EndIf
  \EndFor
\EndFor
\State 
\State $\mathit{precision} \gets \frac{|found|}{|true|}$
\end{algorithmic}
\caption{Algorithm to calculate precision with arbitrary offset between 
	two sets of segments, each segment being a list of tokens.
	\texttt{LCS\_LENGTH} calculates the length of the 
	longest common subsequence of two sequences.  
	We use a solution presented in
	\citep{hunt1977fast}.
	Recall is equivalently calculated, but with different inputs. 
	}
\label{alg:precision_offset}
\end{figure}

- we evaluate in two ways \\
- we measure precision, recall and f1-score of the retrieved segments \\
- also, since we're dealing with sequences, we measure standard measures with 
offset, where we allow for mistakes of arbitrary length \\
- we report results with allowed offset of two \\
- the measure indicates when the extracted segment is close to the desired one, since
having exact segments might not always be important, for example: 
the post 
``\emph{Nothing can bring peace to this earth. Its a great idea to try and
push for world peace, but it will never happen...}''
contains three segments, one of them being
``\emph{but it will never happen...}'', but if the output is 
``\textit{world peace but it will never happen...}
this would even provide additional context 
\\
- the offset precision calculation is outlined in 
figure~\ref{alg:precision_offset} \\
